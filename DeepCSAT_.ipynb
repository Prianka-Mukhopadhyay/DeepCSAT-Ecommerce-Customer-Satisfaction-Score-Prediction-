{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prianka-Mukhopadhyay/DeepCSAT-Ecommerce-Customer-Satisfaction-Score-Prediction-/blob/main/DeepCSAT_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DeepCSAT – Ecommerce Customer Satisfaction Score Prediction**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Prianka Mukhopadhyay"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Develope a deep learning model that can accurately predict the CSAT score based on the customer interaction and feed back"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Display all columns (optional, for large datasets)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# For better visuals\n",
        "sns.set(style=\"whitegrid\", palette=\"pastel\", font_scale=1.1)\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load Dataset\n",
        "df = pd.read_csv('/content/eCommerce_Customer_support_data.csv')\n",
        "\n",
        "# Optional: display dataset name or path\n",
        "print(\"Dataset loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Number of Rows: {df.shape[0]}\")\n",
        "print(f\"Number of Columns: {df.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
        "print(\"Missing Values per Column:\\n\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 85,907 records and 20 columns, representing customer interactions and satisfaction survey data from an e-commerce platform.\n",
        "It includes a mix of categorical, numerical, and date/time features such as customer feedback, product details, agent information, and timestamps of various service stages. The key target variable is “CSAT Score” (Customer Satisfaction Score), which appears to be an integer score—likely on a 1–5 scale, indicating how satisfied customers were with their experience.\n",
        "Some columns such as Unique id, Order_id, Agent_name, and Supervisor serve as identifiers or personnel data, while fields like channel_name, category, Sub-category, and Tenure Bucket provide context about the interaction or employee.\n",
        "The dataset also shows a high degree of missing values in certain columns — for example, connected_handling_time, Customer_City, and Product_category — which will need to be handled during preprocessing. There are no duplicate rows, and most data types are object (categorical text) with only a few numerical columns like Item_price, connected_handling_time, and CSAT Score.\n",
        "\n",
        "In summary:\n",
        "\n",
        "Domain: E-commerce customer satisfaction\n",
        "\n",
        "Objective: Predict or understand factors influencing CSAT Score\n",
        "\n",
        "Rows: 85,907\n",
        "\n",
        "Columns: 20\n",
        "\n",
        "Target Variable: CSAT Score\n",
        "\n",
        "Issues Observed: Missing data, mixed data types, some sparse columns"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Columns in the dataset:\\n\")\n",
        "for col in df.columns:\n",
        "    print(col)\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the statistical summary, the dataset contains three numerical variables — Item_price, connected_handling_time, and CSAT Score — along with several categorical features (described earlier).\n",
        "\n",
        "Insights from numerical variables:\n",
        "\n",
        "Item_price:\n",
        "\n",
        "\n",
        "Represents the cost of the product associated with the customer query.\n",
        "\n",
        "Values range from ₹0 to ₹164,999, with an average of around ₹5,660.\n",
        "\n",
        "The high standard deviation (~12,825) suggests a wide variation in product prices, indicating the dataset includes both low-cost and premium items.\n",
        "\n",
        "Some zero values may represent non-purchase queries or missing price information.\n",
        "\n",
        "connected_handling_time:\n",
        "\n",
        "Refers to the time (in seconds) an agent spent handling a customer interaction.\n",
        "\n",
        "Average handling time is around 462 seconds (~7.7 minutes), with a range from 0 to 1,986 seconds (~33 minutes).\n",
        "\n",
        "However, this column has very few valid entries (only 242 non-null values), meaning it’s mostly missing and might not contribute significantly to analysis.\n",
        "\n",
        "CSAT Score:\n",
        "\n",
        "This is the target variable (Customer Satisfaction Score), ranging from 1 to 5.\n",
        "\n",
        "The mean score is 4.24, with the median and 75th percentile both at 5, showing that most customers rated their experience highly.\n",
        "\n",
        "This skew toward higher scores suggests overall positive satisfaction levels among customers.\n",
        "\n",
        "Overall Summary:\n",
        "\n",
        "The dataset includes a mix of customer interaction, product, and employee information.\n",
        "Numerical features vary widely in scale and completeness.\n",
        "CSAT Score will serve as the dependent variable for modeling, while other columns act as independent predictors."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values for each variable\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].nunique()} unique values\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "\n",
        "# 1. Drop irrelevant or identifier columns\n",
        "df.drop(['Unique id', 'Order_id', 'Agent_name', 'Supervisor', 'Manager'], axis=1, inplace=True)\n",
        "\n",
        "# 2. Drop columns with too many missing values (e.g., >70%)\n",
        "df.drop(['connected_handling_time'], axis=1, inplace=True)\n",
        "\n",
        "# 3. Handle missing values for remaining columns\n",
        "# Fill categorical missing with 'Unknown' and numeric with median\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col].fillna('Unknown', inplace=True)\n",
        "\n",
        "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# 4. Convert date columns to datetime\n",
        "date_cols = ['order_date_time', 'Issue_reported at', 'issue_responded', 'Survey_response_Date']\n",
        "for col in date_cols:\n",
        "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# 5. Create new time-based features (optional)\n",
        "df['response_time_mins'] = (df['issue_responded'] - df['Issue_reported at']).dt.total_seconds() / 60\n",
        "df['survey_delay_days'] = (df['Survey_response_Date'] - df['issue_responded']).dt.days\n",
        "\n",
        "# Fill NaNs created in new features\n",
        "df['response_time_mins'].fillna(df['response_time_mins'].median(), inplace=True)\n",
        "df['survey_delay_days'].fillna(df['survey_delay_days'].median(), inplace=True)\n",
        "\n",
        "# Verify cleaning\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the data wrangling process, I performed several cleaning and transformation steps to make the dataset analysis-ready:\n",
        "\n",
        "-Removed irrelevant columns such as unique identifiers (Unique id, Order_id) and personnel names (Agent_name, Supervisor, Manager) that do not contribute to prediction.\n",
        "\n",
        "-Dropped columns with excessive missing values (e.g., connected_handling_time had >99% null values).\n",
        "\n",
        "-Handled missing data by replacing missing categorical values with 'Unknown' and filling missing numeric values using the median.\n",
        "\n",
        "-Converted date columns into proper datetime format to enable time-based analysis.\n",
        "\n",
        "-Engineered new features such as response time and survey delay to capture customer experience dynamics.\n",
        "\n",
        "-Verified there were no duplicates and data types were consistent after processing.\n",
        "\n",
        "Insights found:\n",
        "\n",
        "*   A large portion of missing data was related to customer or order details — indicating that not all customer queries are tied to purchases.\n",
        "*   Most CSAT Scores are likely high (e.g., 4–5 range), suggesting generally good service quality.\n",
        "* Time-based fields can help explore how quicker responses might lead to higher CSAT scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 -Distribution of CSAT Score"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 1: Distribution of CSAT Score\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x='CSAT Score', palette='viridis',hue='CSAT Score', legend=False)\n",
        "plt.title(\"Distribution of CSAT Scores\")\n",
        "plt.xlabel(\"CSAT Score (1 = Low Satisfaction, 5 = High Satisfaction)\")\n",
        "plt.ylabel(\"Number of Customers\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A countplot is the most effective way to visualize how customer satisfaction scores are distributed across all responses. It instantly shows the balance or imbalance in ratings (e.g., whether most customers are happy or dissatisfied)."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers gave a CSAT Score of 5, indicating very high satisfaction levels. A smaller portion rated 1 or 4, and very few rated 2 or 3. This suggests that customer experience is generally positive, but there’s a small dissatisfied segment that may require attention."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact: Yes. Knowing that the majority of customers are satisfied helps reinforce that current processes are working well.\n",
        "\n",
        " Potential Negative Insight: However, the presence of low scores (1s and 2s) reveals gaps in consistency — those interactions should be analyzed to identify problem categories, agents, or shifts."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2– CSAT by Channel"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 2: CSAT by Communication Channel\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(data=df, x='channel_name', y='CSAT Score', estimator=np.mean, errorbar=None, palette='pastel', hue='channel_name', legend=False)\n",
        "plt.title(\"Average CSAT Score by Communication Channel\")\n",
        "plt.xlabel(\"Channel Name\")\n",
        "plt.ylabel(\"Average CSAT Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart comparing CSAT across channels (Outcall, Inbound, Email) highlights whether the mode of communication affects customer satisfaction — a key operational insight for call centers and support teams."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customers interacting through Outcall and Inbound channels show slightly higher satisfaction (around 4.2–4.3) than Email interactions (~3.8). This suggests that real-time communication leads to better customer experiences compared to asynchronous channels like email."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact: The insight can help management prioritize live communication channels (calls) for sensitive or complex issues, improving customer perception.\n",
        "\n",
        " Potential Negative Insight: The relatively lower email CSAT score indicates delayed or less personalized responses, which might hurt brand satisfaction if not improved."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 – CSAT by Category"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 3: CSAT by Category\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(data=df, x='category', y='CSAT Score', estimator=np.mean, errorbar=None, palette='Set2', hue='category', legend=False)\n",
        "plt.title(\"Average CSAT Score by Category\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel(\"Issue Category\")\n",
        "plt.ylabel(\"Average CSAT Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing satisfaction by issue category reveals which types of customer problems are handled well and which need improvement. It’s essential for targeted process optimization."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most categories maintain an average CSAT above 4, showing stable service quality. However, categories like “App/Website” and possibly “Cancellation” show relatively lower satisfaction (around 3.5–4.0). This suggests that technical or process-driven issues may frustrate customers more than product- or refund-related ones."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact: Teams can focus improvement efforts on low-performing categories (App/Website or Cancellation) — e.g., fixing bugs, simplifying cancellation workflows, or better training for digital support agents.\n",
        "\n",
        " Potential Negative Insight: Persistent low CSAT in technical issue categories might lead to customer churn if not addressed promptly."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 – CSAT by Sub-Category (Top 10 Only)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 4: CSAT by Sub-Category (Top 10)\n",
        "top_subcats = df['Sub-category'].value_counts().head(10).index\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(data=df[df['Sub-category'].isin(top_subcats)],\n",
        "            x='Sub-category', y='CSAT Score', estimator=np.mean, errorbar=None, palette='cool', hue='Sub-category', legend=False)\n",
        "plt.title(\"Average CSAT Score by Top 10 Sub-Categories\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel(\"Sub-Category\")\n",
        "plt.ylabel(\"Average CSAT Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Item Price vs CSAT (Boxplot)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 5: Item Price vs CSAT\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(data=df, x='CSAT Score', y='Item_price', palette='mako', hue='CSAT Score', legend=False)\n",
        "plt.title(\"Item Price vs CSAT Score\")\n",
        "plt.xlabel(\"CSAT Score\")\n",
        "plt.ylabel(\"Item Price\")\n",
        "plt.yscale('log')  # Log scale for better visualization\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 – CSAT by Agent Shift"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 6: CSAT by Agent Shift\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(data=df, x='Agent Shift', y='CSAT Score', estimator=np.mean, errorbar=None, palette='crest', hue='Agent Shift', legend=False)\n",
        "plt.title(\"Average CSAT Score by Agent Shift\")\n",
        "plt.xlabel(\"Agent Shift\")\n",
        "plt.ylabel(\"Average CSAT Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 7: Correlation Heatmap\n",
        "# plt.figure(figsize=(6,5))\n",
        "# sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "# plt.title(\"Correlation Heatmap of Numerical Variables\")\n",
        "# plt.show()\n",
        "# Chart 7: Correlation Heatmap (fixed)\n",
        "plt.figure(figsize=(6,5))\n",
        "\n",
        "# Select only numeric columns for correlation\n",
        "corr = df.select_dtypes(include=['int64', 'float64']).corr()\n",
        "\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap of Numerical Variables\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 8: Pairplot of Numeric Variables\n",
        "sns.pairplot(df[['Item_price', 'CSAT Score']], diag_kind='kde', corner=True)\n",
        "plt.suptitle(\"Pairplot of Numeric Variables\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 – Does Communication Channel Affect Customer Satisfaction?"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H₀ (Null Hypothesis): There is no significant difference in mean CSAT scores across communication channels.\n",
        "\n",
        "H₁ (Alternate Hypothesis): There is a significant difference in mean CSAT scores across communication channels."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Create groups by communication channel\n",
        "groups = [df[df['channel_name'] == ch]['CSAT Score'] for ch in df['channel_name'].unique()]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "stat, p = stats.f_oneway(*groups)\n",
        "\n",
        "print(\"ANOVA Test for CSAT across Channels\")\n",
        "print(f\"F-statistic: {stat:.3f}\")\n",
        "print(f\"P-value: {p:.5f}\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Way ANOVA (Analysis of Variance)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the independent variable (channel_name) has three or more groups, and we want to check if the mean CSAT Score differs significantly among them. ANOVA is suitable for comparing means across multiple categories when the dependent variable is numeric and approximately continuous."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 - Does Agent Shift Affect Customer Satisfaction?"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H₀ (Null Hypothesis):\n",
        "There is no significant difference in mean CSAT scores across different agent shifts (Agent Shift).\n",
        "\n",
        "H₁ (Alternate Hypothesis):\n",
        "There is a significant difference in mean CSAT scores across different agent shifts."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Create groups by Agent Shift\n",
        "groups = [df[df['Agent Shift'] == shift]['CSAT Score'] for shift in df['Agent Shift'].unique()]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "stat, p = stats.f_oneway(*groups)\n",
        "\n",
        "print(\"ANOVA Test for CSAT across Agent Shifts\")\n",
        "print(f\"F-statistic: {stat:.3f}\")\n",
        "print(f\"P-value: {p:.5f}\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Way ANOVA"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the independent variable (Agent Shift) has more than two categorical levels (e.g., Morning, Evening, Night). ANOVA is ideal for testing whether the average CSAT Score varies across these groups."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 — Does the Type of Issue (Category) Affect Customer Satisfaction?"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H₀ (Null Hypothesis):\n",
        "There is no significant difference in mean CSAT scores among the different issue categories (category).\n",
        "\n",
        "H₁ (Alternate Hypothesis):\n",
        "There is a significant difference in mean CSAT scores among the different issue categories."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Create groups by Issue Category\n",
        "groups = [df[df['category'] == cat]['CSAT Score'] for cat in df['category'].unique()]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "stat, p = stats.f_oneway(*groups)\n",
        "\n",
        "print(\"ANOVA Test for CSAT across Issue Categories\")\n",
        "print(f\"F-statistic: {stat:.3f}\")\n",
        "print(f\"P-value: {p:.5f}\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Way ANOVA (Analysis of Variance)"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we’re comparing mean CSAT Scores across multiple categories of customer issues (like Returns, Cancellation, Product Queries, etc.). ANOVA helps identify whether any category group differs significantly in satisfaction levels."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Fill categorical nulls with 'Unknown'\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Fill numeric nulls with median\n",
        "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "print(\"✅ Missing values handled successfully.\")\n",
        "df.isnull().sum().sum()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used categorical imputation with 'Unknown' to preserve rows and indicate missing information as a separate category.\n",
        "\n",
        "For numeric columns, I used median imputation because it’s robust against outliers and doesn’t distort data distribution like the mean can.\n",
        "\n",
        "These techniques ensure no loss of data while keeping statistical properties stable."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Using IQR (Interquartile Range) method for numeric columns\n",
        "numeric_cols = ['Item_price']  # Only numeric field with large spread\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    df[col] = np.where(df[col] > upper, upper,\n",
        "                       np.where(df[col] < lower, lower, df[col]))\n",
        "\n",
        "print(\"✅ Outliers handled using IQR capping.\")\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the IQR (Interquartile Range) method to detect and cap extreme outliers in numerical columns like Item_price.\n",
        "\n",
        "This method limits the influence of extreme values without dropping data points, which is essential for large business datasets where every record is valuable."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "for col in cat_cols:\n",
        "    df[col] = encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "print(\"✅ Categorical columns encoded successfully.\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Label Encoding because most categorical features (like channel_name, category, Agent Shift) are non-ordinal with a moderate number of unique values.\n",
        "Label encoding efficiently converts text labels into numeric form required for model training and works well for ANN models."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "This project doesn’t involve direct text mining or sentiment analysis.\n",
        "\n",
        "The Customer Remarks column was excluded from the modeling stage since it contains unstructured comments that require separate NLP preprocessing (like tokenization and sentiment scoring), which is outside this project’s current scope."
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Convert datetime columns to datetime dtype (if not already)\n",
        "date_cols = ['order_date_time', 'Issue_reported at', 'issue_responded', 'Survey_response_Date']\n",
        "for col in date_cols:\n",
        "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# Create new engineered features\n",
        "df['response_time_mins'] = (df['issue_responded'] - df['Issue_reported at']).dt.total_seconds() / 60\n",
        "df['survey_delay_days'] = (df['Survey_response_Date'] - df['issue_responded']).dt.days\n",
        "\n",
        "# Fill NaN in new columns\n",
        "df['response_time_mins'].fillna(df['response_time_mins'].median(), inplace=True)\n",
        "df['survey_delay_days'].fillna(df['survey_delay_days'].median(), inplace=True)\n",
        "\n",
        "print(\"✅ Feature manipulation done successfully.\")\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feedback Sentiment Feature Extraction (before dropping Customer Remarks) ---\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np\n",
        "\n",
        "if 'Customer Remarks' in df.columns:\n",
        "    # Train sentiment submodel on available remarks\n",
        "    text_df = df[df['Customer Remarks'].notnull()].copy()\n",
        "\n",
        "    tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
        "    X_text = tfidf.fit_transform(text_df['Customer Remarks'].astype(str))\n",
        "\n",
        "    y_text = text_df['CSAT Score']\n",
        "\n",
        "    sentiment_model = Ridge(alpha=1.0)\n",
        "    sentiment_model.fit(X_text, y_text)\n",
        "\n",
        "    # Predict feedback sentiment for all rows (fill missing with average)\n",
        "    all_text = df['Customer Remarks'].fillna('')\n",
        "    text_pred = sentiment_model.predict(tfidf.transform(all_text.astype(str)))\n",
        "\n",
        "    df['feedback_sentiment'] = np.where(df['Customer Remarks'].isnull(),\n",
        "                                        np.mean(y_text),\n",
        "                                        text_pred)\n",
        "    print(\"✅ Created feedback_sentiment feature from Customer Remarks.\")\n",
        "else:\n",
        "    print(\"⚠️ 'Customer Remarks' column not found, skipping feedback sentiment feature.\")\n"
      ],
      "metadata": {
        "id": "_U5nkh98GWLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Drop irrelevant columns that don’t contribute to prediction\n",
        "drop_cols = ['Unique id', 'Order_id', 'Customer Remarks', 'Agent_name', 'Supervisor', 'Manager']\n",
        "df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
        "\n",
        "print(\"✅ Dropped irrelevant and non-predictive features.\")\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a manual relevance-based feature selection approach, supported by correlation analysis and business understanding.\n",
        "Features with high correlation to each other or no logical connection to customer satisfaction (e.g., IDs, agent names) were dropped to prevent overfitting.\n",
        "This ensures the model focuses only on variables that add genuine predictive value."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "channel_name – type of communication (e.g., Inbound/Outcall) can directly impact satisfaction.\n",
        "\n",
        "category & Sub-category – issue type often determines customer frustration level.\n",
        "\n",
        "Agent Shift & Tenure Bucket – reflect agent experience and workload time.\n",
        "\n",
        "response_time_mins – faster responses likely lead to higher satisfaction.\n",
        "\n",
        "Item_price – higher-value purchases may lead to different expectations.\n",
        "\n",
        "These variables have both business relevance and measurable correlation with CSAT scores."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was needed.\n",
        "\n",
        "Numerical columns like Item_price and the engineered features (response_time_mins, survey_delay_days) were on different scales.\n",
        "\n",
        "I used log transformation for skewed variables like Item_price to reduce the impact of extreme values, followed by standard scaling.\n",
        "\n",
        "This makes data more normally distributed and improves model learning stability."
      ],
      "metadata": {
        "id": "02UqqRH7FFZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your Data\n",
        "import numpy as np\n",
        "\n",
        "df['Item_price'] = np.log1p(df['Item_price'])  # log transformation\n",
        "print(\"✅ Log transformation applied to Item_price.\")\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[['Item_price']] = scaler.fit_transform(df[['Item_price']])\n",
        "\n",
        "print(\"✅ Data scaling done using StandardScaler.\")\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler to bring numerical columns onto a standard scale (mean = 0, std = 1).\n",
        "Neural networks are sensitive to feature magnitudes, so scaling helps the optimizer converge faster and avoids bias toward larger-valued features."
      ],
      "metadata": {
        "id": "ppuBjJfECJNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction was not required in this dataset because the total number of input features after encoding and selection was manageable.\n",
        "\n",
        "Additionally, removing dimensions might cause information loss since most features have distinct business meaning.\n",
        "\n",
        "If the dataset had high multicollinearity or hundreds of features, I would have used PCA (Principal Component Analysis) to reduce redundancy."
      ],
      "metadata": {
        "id": "N4Bz7YDNFXfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('CSAT Score', axis=1)\n",
        "y = df['CSAT Score']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"✅ Data split successfully.\")\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "3kwohwvs_QIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # List of datetime columns you want to drop\n",
        "# datetime_cols = ['order_date_time', 'Issue_reported at', 'issue_responded', 'Survey_response_Date']\n",
        "\n",
        "# # Convert back to DataFrame if needed\n",
        "# if isinstance(X_train, np.ndarray):\n",
        "#     X_train = pd.DataFrame(X_train)\n",
        "#     X_test = pd.DataFrame(X_test)\n",
        "\n",
        "# # Drop datetime columns if they exist\n",
        "# X_train = X_train.drop(columns=[col for col in datetime_cols if col in X_train.columns], errors='ignore')\n",
        "# X_test = X_test.drop(columns=[col for col in datetime_cols if col in X_test.columns], errors='ignore')\n",
        "\n",
        "# # Optional: convert back to NumPy arrays for model\n",
        "# X_train = X_train.values\n",
        "# X_test = X_test.values\n",
        "\n",
        "# print(\"✅ Dropped datetime columns.\")\n",
        "# print(\"X_train shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "rHyFug57JBxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X = df.drop('CSAT Score', axis=1)\n",
        "# y = df['CSAT Score']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# print(\"✅ Data split successfully.\")\n",
        "# print(\"Training set shape:\", X_train.shape)\n",
        "# print(\"Testing set shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 train-test split to ensure that the model learns from sufficient data while maintaining a fair portion for validation.\n",
        "\n",
        "The 20% test data helps evaluate how well the model generalizes to unseen data and reduces the risk of overfitting."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset is somewhat imbalanced — most customers gave a CSAT Score of 5, while lower scores (1–3) are underrepresented.\n",
        "\n",
        "This imbalance could bias the model toward predicting higher satisfaction."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ANN Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANN Basic Model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop datetime columns before model training\n",
        "datetime_cols = ['order_date_time', 'Issue_reported at', 'issue_responded', 'Survey_response_Date']\n",
        "X_train = X_train.drop(columns=[col for col in datetime_cols if col in X_train.columns], errors='ignore')\n",
        "X_test = X_test.drop(columns=[col for col in datetime_cols if col in X_test.columns], errors='ignore')\n",
        "\n",
        "print(\"✅ Dropped datetime columns before model training.\")\n",
        "print(\"X_train shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "tZKXyL9z-otK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild ANN model after dropping columns\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # updated input shape\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "jFVWQIoMI5ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "non_numeric_cols = X_train.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "zlQ2hzd6-obf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict on Test Data\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "# Evaluation Metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))   # instead of squared=False\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"MAE:  {mae:.3f}\")\n",
        "print(f\"RMSE: {rmse:.3f}\")\n",
        "print(f\"R²:   {r2:.3f}\")\n"
      ],
      "metadata": {
        "id": "tmIEnlhHAFac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title(\"Model Training vs Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pTJMV5YdIjDs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Inference**"
      ],
      "metadata": {
        "id": "x1p7hqvZrsxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ANN model achieved an MAE of 1.058, RMSE of 1.365, and R² of 0.014 on the test set.\n",
        "\n",
        "These results indicate that the model currently has limited predictive capability, capturing only a small portion of the variability in customer satisfaction.\n",
        "\n",
        "The high error suggests that CSAT scores are influenced by complex, possibly non-linear or unrecorded factors.\n",
        "\n",
        "Future improvement could include feature enrichment, better class balancing, and testing alternate algorithms like Random Forest or XGBoost, which are often more effective for structured data."
      ],
      "metadata": {
        "id": "1HCZiiYNIb8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MODEL IMPROVEMENT"
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# IMPROVED DEEPCSAT MODEL - COMPREHENSIVE APPROACH\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: FEATURE SCALING (Critical for ANN)\n",
        "# ============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 1: Scaling Features\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use RobustScaler (better for outliers) or StandardScaler\n",
        "scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"✅ Features scaled using RobustScaler\")\n",
        "print(f\"Training shape: {X_train_scaled.shape}\")\n",
        "print(f\"Test shape: {X_test_scaled.shape}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: IMPROVED ANN ARCHITECTURE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: Building Improved ANN Model\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear any previous models\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Build improved model with BatchNormalization and better architecture\n",
        "model_ann = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# Use custom learning rate\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_ann.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "print(\"\\n📊 Model Architecture:\")\n",
        "model_ann.summary()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: ADVANCED CALLBACKS\n",
        "# ============================================================================\n",
        "\n",
        "# Early stopping with more patience\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Reduce learning rate when stuck\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: TRAIN IMPROVED ANN\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: Training Improved ANN Model\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = model_ann.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,  # More epochs with early stopping\n",
        "    batch_size=64,  # Larger batch size\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: EVALUATE ANN\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: Evaluating Improved ANN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_pred_ann = model_ann.predict(X_test_scaled, verbose=0).flatten()\n",
        "\n",
        "mae_ann = mean_absolute_error(y_test, y_pred_ann)\n",
        "rmse_ann = np.sqrt(mean_squared_error(y_test, y_pred_ann))\n",
        "r2_ann = r2_score(y_test, y_pred_ann)\n",
        "\n",
        "print(\"\\n🎯 Improved ANN Results:\")\n",
        "print(f\"MAE:  {mae_ann:.4f}\")\n",
        "print(f\"RMSE: {rmse_ann:.4f}\")\n",
        "print(f\"R²:   {r2_ann:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: ENSEMBLE MODELS (Usually Better for Tabular Data)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 5: Training Ensemble Models\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Random Forest\n",
        "print(\"\\n🌲 Training Random Forest...\")\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"✅ Random Forest Results:\")\n",
        "print(f\"MAE:  {mae_rf:.4f}\")\n",
        "print(f\"RMSE: {rmse_rf:.4f}\")\n",
        "print(f\"R²:   {r2_rf:.4f}\")\n",
        "\n",
        "# XGBoost\n",
        "print(\"\\n🚀 Training XGBoost...\")\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbosity=0\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"✅ XGBoost Results:\")\n",
        "print(f\"MAE:  {mae_xgb:.4f}\")\n",
        "print(f\"RMSE: {rmse_xgb:.4f}\")\n",
        "print(f\"R²:   {r2_xgb:.4f}\")\n",
        "\n",
        "# Gradient Boosting\n",
        "print(\"\\n📈 Training Gradient Boosting...\")\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "\n",
        "print(f\"✅ Gradient Boosting Results:\")\n",
        "print(f\"MAE:  {mae_gb:.4f}\")\n",
        "print(f\"RMSE: {rmse_gb:.4f}\")\n",
        "print(f\"R²:   {r2_gb:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: MODEL COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['Original ANN', 'Improved ANN', 'Random Forest', 'XGBoost', 'Gradient Boosting'],\n",
        "    'MAE': [1.067, mae_ann, mae_rf, mae_xgb, mae_gb],\n",
        "    'RMSE': [1.368, rmse_ann, rmse_rf, rmse_xgb, rmse_gb],\n",
        "    'R²': [0.010, r2_ann, r2_rf, r2_xgb, r2_gb]\n",
        "})\n",
        "\n",
        "results_df = results_df.sort_values('R²', ascending=False)\n",
        "print(\"\\n\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"   MAE: {results_df.iloc[0]['MAE']:.4f}\")\n",
        "print(f\"   RMSE: {results_df.iloc[0]['RMSE']:.4f}\")\n",
        "print(f\"   R²: {results_df.iloc[0]['R²']:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: VISUALIZATIONS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 6: Creating Visualizations\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Plot 1: Training History (ANN)\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "plt.title(\"Improved ANN: Training vs Validation Loss\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Epochs\", fontsize=12)\n",
        "plt.ylabel(\"Loss (MSE)\", fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Model Comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "x_pos = np.arange(len(results_df))\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
        "plt.barh(x_pos, results_df['R²'], color=colors)\n",
        "plt.yticks(x_pos, results_df['Model'])\n",
        "plt.xlabel('R² Score', fontsize=12)\n",
        "plt.title('Model Performance Comparison (R²)', fontsize=14, fontweight='bold')\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Target')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Actual vs Predicted (Best Model)\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Determine best predictions\n",
        "if best_model_name == 'Improved ANN':\n",
        "    best_predictions = y_pred_ann\n",
        "elif best_model_name == 'Random Forest':\n",
        "    best_predictions = y_pred_rf\n",
        "elif best_model_name == 'XGBoost':\n",
        "    best_predictions = y_pred_xgb\n",
        "else:\n",
        "    best_predictions = y_pred_gb\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, best_predictions, alpha=0.5, s=20)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual CSAT', fontsize=12)\n",
        "plt.ylabel('Predicted CSAT', fontsize=12)\n",
        "plt.title(f'Actual vs Predicted - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y_test - best_predictions\n",
        "plt.scatter(best_predictions, residuals, alpha=0.5, s=20)\n",
        "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "plt.xlabel('Predicted CSAT', fontsize=12)\n",
        "plt.ylabel('Residuals', fontsize=12)\n",
        "plt.title(f'Residual Plot - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: FEATURE IMPORTANCE (for tree-based models)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 7: Feature Importance Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if best_model_name in ['Random Forest', 'XGBoost', 'Gradient Boosting']:\n",
        "    if best_model_name == 'Random Forest':\n",
        "        feature_imp = rf_model.feature_importances_\n",
        "    elif best_model_name == 'XGBoost':\n",
        "        feature_imp = xgb_model.feature_importances_\n",
        "    else:\n",
        "        feature_imp = gb_model.feature_importances_\n",
        "\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': feature_imp\n",
        "    }).sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(len(feature_importance_df)), feature_importance_df['Importance'])\n",
        "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'])\n",
        "    plt.xlabel('Importance', fontsize=12)\n",
        "    plt.title(f'Top 15 Feature Importances - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(True, alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(feature_importance_df.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ MODEL IMPROVEMENT COMPLETE!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "cyAdaeavbeYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Multiple Models"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FAST DIAGNOSTIC & MODEL IMPROVEMENT - CSAT PREDICTION\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🔍 STEP 1: DIAGNOSTIC CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check data type and convert if needed\n",
        "print(f\"\\n📊 Data Types:\")\n",
        "print(f\"   X_train type: {type(X_train)}\")\n",
        "print(f\"   X_test type: {type(X_test)}\")\n",
        "print(f\"   y_train type: {type(y_train)}\")\n",
        "\n",
        "# Convert to DataFrame if numpy array\n",
        "if isinstance(X_train, np.ndarray):\n",
        "    print(\"⚠️ Converting numpy arrays to DataFrames...\")\n",
        "    feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
        "    X_train = pd.DataFrame(X_train, columns=feature_names)\n",
        "    X_test = pd.DataFrame(X_test, columns=feature_names)\n",
        "    print(\"✅ Converted to DataFrames\")\n",
        "\n",
        "# Check your data\n",
        "print(f\"\\n📊 Training Data Shape: {X_train.shape}\")\n",
        "print(f\"📊 Test Data Shape: {X_test.shape}\")\n",
        "\n",
        "# Check target\n",
        "if hasattr(y_train, 'value_counts'):\n",
        "    print(f\"📊 Target Distribution:\")\n",
        "    print(y_train.value_counts().sort_index())\n",
        "else:\n",
        "    print(f\"📊 Target Distribution:\")\n",
        "    unique, counts = np.unique(y_train, return_counts=True)\n",
        "    for val, count in zip(unique, counts):\n",
        "        print(f\"   {val}: {count}\")\n",
        "\n",
        "print(f\"\\n📊 Target Statistics:\")\n",
        "print(f\"   Mean: {np.mean(y_train):.2f}\")\n",
        "print(f\"   Std: {np.std(y_train):.2f}\")\n",
        "print(f\"   Min: {np.min(y_train)}\")\n",
        "print(f\"   Max: {np.max(y_train)}\")\n",
        "\n",
        "# Check for constant features\n",
        "constant_features = []\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].nunique() <= 1:\n",
        "        constant_features.append(col)\n",
        "\n",
        "if constant_features:\n",
        "    print(f\"\\n⚠️ Constant features found (will remove): {constant_features}\")\n",
        "    X_train = X_train.drop(columns=constant_features)\n",
        "    X_test = X_test.drop(columns=constant_features)\n",
        "    print(f\"✅ Removed constant features. New shape: {X_train.shape}\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_train = X_train.isnull().sum().sum()\n",
        "missing_test = X_test.isnull().sum().sum()\n",
        "print(f\"\\n📊 Missing values - Train: {missing_train}, Test: {missing_test}\")\n",
        "\n",
        "if missing_train > 0 or missing_test > 0:\n",
        "    print(\"⚠️ Filling missing values with median...\")\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
        "    print(\"✅ Missing values handled\")\n",
        "\n",
        "# Check feature names\n",
        "print(f\"\\n📋 Available Features ({len(X_train.columns)}):\")\n",
        "print(X_train.columns.tolist())\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: BASELINE MODEL - PREDICT MEAN\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📈 STEP 2: BASELINE MODEL (Predict Mean)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "y_pred_baseline = np.full(len(y_test), y_train.mean())\n",
        "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
        "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
        "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
        "\n",
        "print(f\"\\n🎯 Baseline (Always Predict Mean = {y_train.mean():.2f}):\")\n",
        "print(f\"   MAE:  {mae_baseline:.4f}\")\n",
        "print(f\"   RMSE: {rmse_baseline:.4f}\")\n",
        "print(f\"   R²:   {r2_baseline:.4f}\")\n",
        "print(\"\\n⚠️ Any model MUST beat this baseline!\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: SCALE FEATURES (CRITICAL!)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"⚙️ STEP 3: FEATURE SCALING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"✅ Features scaled with StandardScaler\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: QUICK MODELS (NO TUNING)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🚀 STEP 4: TRAINING MULTIPLE MODELS (FAST)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results = []\n",
        "\n",
        "# Model 1: Random Forest (Fast settings)\n",
        "print(\"\\n🌲 Training Random Forest...\")\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"✅ Random Forest:\")\n",
        "print(f\"   MAE:  {mae_rf:.4f}\")\n",
        "print(f\"   RMSE: {rmse_rf:.4f}\")\n",
        "print(f\"   R²:   {r2_rf:.4f}\")\n",
        "results.append(['Random Forest', mae_rf, rmse_rf, r2_rf])\n",
        "\n",
        "# Model 2: Gradient Boosting\n",
        "print(\"\\n📈 Training Gradient Boosting...\")\n",
        "gb = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "\n",
        "print(f\"✅ Gradient Boosting:\")\n",
        "print(f\"   MAE:  {mae_gb:.4f}\")\n",
        "print(f\"   RMSE: {rmse_gb:.4f}\")\n",
        "print(f\"   R²:   {r2_gb:.4f}\")\n",
        "results.append(['Gradient Boosting', mae_gb, rmse_gb, r2_gb])\n",
        "\n",
        "# Model 3: Try XGBoost if available\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    print(\"\\n🚀 Training XGBoost...\")\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "    r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "    print(f\"✅ XGBoost:\")\n",
        "    print(f\"   MAE:  {mae_xgb:.4f}\")\n",
        "    print(f\"   RMSE: {rmse_xgb:.4f}\")\n",
        "    print(f\"   R²:   {r2_xgb:.4f}\")\n",
        "    results.append(['XGBoost', mae_xgb, rmse_xgb, r2_xgb])\n",
        "except:\n",
        "    print(\"⚠️ XGBoost not available, skipping...\")\n",
        "    y_pred_xgb = None\n",
        "\n",
        "# Model 4: Improved ANN\n",
        "print(\"\\n🧠 Training Improved ANN...\")\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model_ann = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model_ann.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
        "\n",
        "history = model_ann.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "y_pred_ann = model_ann.predict(X_test_scaled, verbose=0).flatten()\n",
        "\n",
        "mae_ann = mean_absolute_error(y_test, y_pred_ann)\n",
        "rmse_ann = np.sqrt(mean_squared_error(y_test, y_pred_ann))\n",
        "r2_ann = r2_score(y_test, y_pred_ann)\n",
        "\n",
        "print(f\"✅ Improved ANN:\")\n",
        "print(f\"   MAE:  {mae_ann:.4f}\")\n",
        "print(f\"   RMSE: {rmse_ann:.4f}\")\n",
        "print(f\"   R²:   {r2_ann:.4f}\")\n",
        "results.append(['Improved ANN', mae_ann, rmse_ann, r2_ann])\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: RESULTS COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📊 RESULTS COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['Model', 'MAE', 'RMSE', 'R²'])\n",
        "results_df = results_df.sort_values('R²', ascending=False)\n",
        "\n",
        "print(f\"\\n{'Model':<25} {'MAE':<10} {'RMSE':<10} {'R²':<10}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'Baseline (Mean)':<25} {mae_baseline:<10.4f} {rmse_baseline:<10.4f} {r2_baseline:<10.4f}\")\n",
        "for _, row in results_df.iterrows():\n",
        "    print(f\"{row['Model']:<25} {row['MAE']:<10.4f} {row['RMSE']:<10.4f} {row['R²']:<10.4f}\")\n",
        "\n",
        "best_model = results_df.iloc[0]\n",
        "print(f\"\\n🏆 BEST MODEL: {best_model['Model']}\")\n",
        "print(f\"   MAE:  {best_model['MAE']:.4f}\")\n",
        "print(f\"   RMSE: {best_model['RMSE']:.4f}\")\n",
        "print(f\"   R²:   {best_model['R²']:.4f}\")\n",
        "\n",
        "# Check if we beat baseline\n",
        "if best_model['R²'] > r2_baseline:\n",
        "    improvement = (best_model['R²'] - r2_baseline) / abs(r2_baseline) * 100\n",
        "    print(f\"✅ Improved over baseline by {improvement:.1f}%\")\n",
        "else:\n",
        "    print(\"⚠️ WARNING: Models not beating baseline! Check data quality.\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: DIAGNOSTIC PLOTS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📊 STEP 5: CREATING DIAGNOSTIC PLOTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Plot 1: Target distribution\n",
        "axes[0, 0].hist(y_train, bins=20, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title('Target (CSAT) Distribution', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('CSAT Score')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Model comparison\n",
        "model_names = ['Baseline'] + results_df['Model'].tolist()\n",
        "r2_scores = [r2_baseline] + results_df['R²'].tolist()\n",
        "colors = ['red'] + ['green' if r2 > r2_baseline else 'orange' for r2 in results_df['R²']]\n",
        "\n",
        "axes[0, 1].barh(model_names, r2_scores, color=colors)\n",
        "axes[0, 1].set_xlabel('R² Score')\n",
        "axes[0, 1].set_title('Model Performance (R²)', fontweight='bold')\n",
        "axes[0, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 3: MAE comparison\n",
        "mae_scores = [mae_baseline] + results_df['MAE'].tolist()\n",
        "axes[0, 2].barh(model_names, mae_scores, color=colors)\n",
        "axes[0, 2].set_xlabel('MAE')\n",
        "axes[0, 2].set_title('Model Performance (MAE - Lower is Better)', fontweight='bold')\n",
        "axes[0, 2].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Determine best predictions\n",
        "if best_model['Model'] == 'Random Forest':\n",
        "    best_pred = y_pred_rf\n",
        "elif best_model['Model'] == 'Gradient Boosting':\n",
        "    best_pred = y_pred_gb\n",
        "elif best_model['Model'] == 'XGBoost':\n",
        "    best_pred = y_pred_xgb\n",
        "else:\n",
        "    best_pred = y_pred_ann\n",
        "\n",
        "# Plot 4: Actual vs Predicted\n",
        "axes[1, 0].scatter(y_test, best_pred, alpha=0.5, s=20)\n",
        "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1, 0].set_xlabel('Actual CSAT')\n",
        "axes[1, 0].set_ylabel('Predicted CSAT')\n",
        "axes[1, 0].set_title(f'Actual vs Predicted - {best_model[\"Model\"]}', fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Residuals\n",
        "residuals = y_test - best_pred\n",
        "axes[1, 1].scatter(best_pred, residuals, alpha=0.5, s=20)\n",
        "axes[1, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1, 1].set_xlabel('Predicted CSAT')\n",
        "axes[1, 1].set_ylabel('Residuals')\n",
        "axes[1, 1].set_title(f'Residual Plot - {best_model[\"Model\"]}', fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Prediction error distribution\n",
        "axes[1, 2].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[1, 2].set_xlabel('Prediction Error')\n",
        "axes[1, 2].set_ylabel('Frequency')\n",
        "axes[1, 2].set_title('Distribution of Prediction Errors', fontweight='bold')\n",
        "axes[1, 2].axvline(x=0, color='r', linestyle='--', lw=2)\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: FEATURE IMPORTANCE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🔍 STEP 6: FEATURE IMPORTANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if best_model['Model'] in ['Random Forest', 'Gradient Boosting', 'XGBoost']:\n",
        "    if best_model['Model'] == 'Random Forest':\n",
        "        importances = rf.feature_importances_\n",
        "    elif best_model['Model'] == 'Gradient Boosting':\n",
        "        importances = gb.feature_importances_\n",
        "    else:\n",
        "        importances = xgb_model.feature_importances_\n",
        "\n",
        "    feat_imp = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\n🔝 Top 10 Most Important Features:\")\n",
        "    print(feat_imp.head(10).to_string(index=False))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    top_features = feat_imp.head(15)\n",
        "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
        "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title(f'Top 15 Features - {best_model[\"Model\"]}', fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(True, alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL DIAGNOSTIC\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯 DIAGNOSTIC SUMMARY & RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if best_model['R²'] < 0.3:\n",
        "    print(\"\\n⚠️ LOW R² SCORE - POSSIBLE CAUSES:\")\n",
        "    print(\"   1. CSAT may be heavily influenced by factors NOT in your data\")\n",
        "    print(\"   2. Data quality issues (too much missing data)\")\n",
        "    print(\"   3. Features don't have strong predictive power\")\n",
        "    print(\"   4. Target variable might be too subjective/random\")\n",
        "    print(\"\\n💡 RECOMMENDATIONS:\")\n",
        "    print(\"   - Add more features (customer sentiment, product reviews, chat logs)\")\n",
        "    print(\"   - Try treating this as classification (predict 1-5 stars)\")\n",
        "    print(\"   - Check for data leakage (is target accidentally in features?)\")\n",
        "    print(\"   - Consider that CSAT might just be hard to predict!\")\n",
        "\n",
        "elif best_model['R²'] < 0.5:\n",
        "    print(\"\\n✅ MODERATE R² SCORE - ROOM FOR IMPROVEMENT:\")\n",
        "    print(\"\\n💡 NEXT STEPS:\")\n",
        "    print(\"   - Try hyperparameter tuning (but use RandomizedSearchCV)\")\n",
        "    print(\"   - Create interaction features\")\n",
        "    print(\"   - Try ensemble methods (stack multiple models)\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n✅ GOOD R² SCORE!\")\n",
        "    print(\"\\n💡 NEXT STEPS:\")\n",
        "    print(\"   - Fine-tune hyperparameters\")\n",
        "    print(\"   - Deploy model for production use\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "mqhwzdNNjHZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification and Ordinal Regression"
      ],
      "metadata": {
        "id": "U-1EO7IIqt-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "!pip install mord\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
        "import mord\n",
        "\n",
        "# ======================\n",
        "# 1. Random Forest Classifier\n",
        "# ======================\n",
        "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"=== Random Forest Classifier ===\")\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"MAE:\", mae_rf)\n",
        "print(\"RMSE:\", rmse_rf)\n",
        "print(\"R²:\", r2_rf)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# ======================\n",
        "# 2. Ordinal Regression using mord\n",
        "# ======================\n",
        "# Install mord if not already: pip install mord\n",
        "ordinal_model = mord.LogisticIT()  # logistic ordinal regression\n",
        "ordinal_model.fit(X_train, y_train)\n",
        "y_pred_ord = ordinal_model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "mae_ord = mean_absolute_error(y_test, y_pred_ord)\n",
        "rmse_ord = np.sqrt(mean_squared_error(y_test, y_pred_ord))\n",
        "r2_ord = r2_score(y_test, y_pred_ord)\n",
        "accuracy_ord = accuracy_score(y_test, y_pred_ord)\n",
        "\n",
        "print(\"\\n=== Ordinal Regression ===\")\n",
        "print(\"Accuracy:\", accuracy_ord)\n",
        "print(\"MAE:\", mae_ord)\n",
        "print(\"RMSE:\", rmse_ord)\n",
        "print(\"R²:\", r2_ord)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_ord))\n"
      ],
      "metadata": {
        "id": "hWX_XpFDoRJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Inference**"
      ],
      "metadata": {
        "id": "Z5_QkIcatMWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predictive Performance:**\n",
        "\n",
        "All classification and ordinal regression models had higher MAE/RMSE and negative R², indicating they performed worse than the baseline ANN.\n",
        "\n",
        "This suggests that the models were unable to capture the patterns in the dataset as effectively as the ANN.\n",
        "Impact of Feature Set:\n",
        "\n",
        "The structured features available (numeric/categorical features like Item_price, response_time_mins, Agent Shift) provide limited predictive power for CSAT.\n",
        "CSAT seems to be influenced by factors not captured in the dataset, such as customer sentiment, product quality, or subjective perceptions.\n",
        "\n",
        "**Ordinal Regression Insights:**\n",
        "\n",
        "Ordinal regression respected the ordered nature of CSAT scores but did not improve MAE or RMSE, implying that the relationships between features and CSAT are weak or non-linear, which the model could not effectively capture.\n",
        "\n",
        "**Classification Observations:**\n",
        "\n",
        "Random Forest, XGBoost, and Gradient Boosting achieved accuracy ~38–45%, showing that the models are only slightly better than random guessing for 5 classes.\n",
        "This reinforces the idea that CSAT prediction from structured features alone is very challenging.\n",
        "\n",
        "**Comparison to ANN:**\n",
        "\n",
        "The original ANN consistently produced lower MAE and RMSE and slightly positive R², indicating it is better suited to model the subtle numerical relationships in the features, even if improvements are modest.\n",
        "\n",
        "**Baseline Comparison:**\n",
        "\n",
        "Some models performed worse than the baseline (predicting the mean CSAT), confirming that simple tree ensembles and ordinal regression are not sufficient for this dataset.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "“The classification and ordinal regression models failed to outperform the original ANN in predicting CSAT scores, highlighting that the available structured features have limited predictive power and that the ANN’s ability to capture subtle relationships makes it the best model for this dataset.”"
      ],
      "metadata": {
        "id": "eAu99RhYstYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test your model with a new feedback statement ---\n",
        "sample_feedback = [\"Delivery was late but the agent was polite and solved my issue quickly\"]\n",
        "\n",
        "sample_tfidf = tfidf.transform(pd.Series(sample_feedback).astype(str))\n",
        "sample_sentiment = sentiment_model.predict(sample_tfidf)[0]\n",
        "\n",
        "sample_data = {\n",
        "    'channel_name': [0],\n",
        "    'category': [0],\n",
        "    'Sub-category': [0],\n",
        "    'Customer_City': [0],\n",
        "    'Product_category': [0],\n",
        "    'Item_price': [np.mean(df['Item_price'])],\n",
        "    'Tenure Bucket': [0],\n",
        "    'Agent Shift': [0],\n",
        "    'response_time_mins': [np.median(df['response_time_mins'])],\n",
        "    'survey_delay_days': [np.median(df['survey_delay_days'])],\n",
        "    'feedback_sentiment': [sample_sentiment]\n",
        "}\n",
        "\n",
        "sample_df = pd.DataFrame(sample_data)\n",
        "sample_scaled = scaler.transform(sample_df)\n",
        "predicted_csat = model.predict(sample_scaled)\n",
        "print(f\"Predicted CSAT Score: {predicted_csat[0][0]:.2f}\")\n"
      ],
      "metadata": {
        "id": "xF8z73q-MIqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this study, multiple models were evaluated to predict customer satisfaction (CSAT) scores using structured e-commerce interaction data. The original ANN model consistently outperformed ensemble models (Random Forest, Gradient Boosting, XGBoost) and improved ANN variants, achieving the lowest MAE and RMSE with a slightly positive R².\n",
        "The results indicate that while structured features provide some predictive power, CSAT scores are inherently noisy and influenced by factors outside the dataset. Future work should focus on adding richer features (customer sentiment, product reviews), experimenting with classification or ordinal regression approaches, and deploying the best ANN model for real-time predictions with ongoing monitoring."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}